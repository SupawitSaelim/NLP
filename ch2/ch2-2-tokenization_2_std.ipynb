{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPGnycdeSRsTp9/8eiYoEY+"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["**Advanced tokenization** with **NLTK** and **regex**\n","**Regex groups** using or **\"|\"**\n","*   **OR** method is represented using **|**\n","*   Define  **a group** using **( )**\n","*   Define explicit character ranges using [ ]\n","\n"],"metadata":{"id":"LjAnyc6b67qq"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"VRG6OViq51Dc"},"outputs":[],"source":["import re"]},{"cell_type":"markdown","source":["**Ex.1** match sequences of digits or sequences of word characters."],"metadata":{"id":"A5xOrxypq-6U"}},{"cell_type":"code","source":["match_digits_and_words = ('(\\d+|\\w+)')\n","re.findall(match_digits_and_words, 'He has 11 cats')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PvF02AtTVX01","executionInfo":{"status":"ok","timestamp":1721219455493,"user_tz":-420,"elapsed":435,"user":{"displayName":"Supaporn Simcharoen","userId":"08684475074805078308"}},"outputId":"7b6591af-a72b-4d9e-bf5d-335f8037f915"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['He', 'has', '11', 'cats']"]},"metadata":{},"execution_count":2}]},{"cell_type":"markdown","source":["**Ex.2** match one or more occurrences of lowercase letters, digits, or spaces."],"metadata":{"id":"7ZtupiR-100x"}},{"cell_type":"code","source":["my_str = 'match lowercase spaces nums like 12, but no commas'\n","match_str = ('[a-z0-9 ]+')\n","re.match(match_str, my_str)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"A6crjTxc1KJO","executionInfo":{"status":"ok","timestamp":1721222899389,"user_tz":-420,"elapsed":4,"user":{"displayName":"Supaporn Simcharoen","userId":"08684475074805078308"}},"outputId":"ae7c1def-da36-4431-aa51-ae57ec205181"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<re.Match object; span=(0, 35), match='match lowercase spaces nums like 12'>"]},"metadata":{},"execution_count":8}]},{"cell_type":"markdown","source":["**Let's practice!**"],"metadata":{"id":"Mxk-eBs166a-"}},{"cell_type":"code","source":["from nltk.tokenize import regexp_tokenize"],"metadata":{"id":"6jnyzKRI8IOG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","*   Given the following string, which of the below patterns is the best tokenizer? If possible, you want to retain sentence punctuation as separate tokens, but have '#1' remain a single token.\n","\n","\n"],"metadata":{"id":"bJc-QqwH72Cf"}},{"cell_type":"code","source":["my_string = \"SOLDIER #1: Found them? In Mercea? The coconut's tropical!\"\n","pattern =\n","print(regexp_tokenize(my_string, pattern))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SpPXT1Uc8AVW","executionInfo":{"status":"ok","timestamp":1721224001767,"user_tz":-420,"elapsed":7,"user":{"displayName":"Supaporn Simcharoen","userId":"08684475074805078308"}},"outputId":"968eebea-591e-425b-e8a7-8308a65c72ab"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['SOLDIER', '#1', 'Found', 'them', '?', 'In', 'Mercea', '?', 'The', 'coconut', 's', 'tropical', '!']\n"]}]},{"cell_type":"markdown","source":["\n","\n","*  **Twitter** is a frequently used source for NLP text and tasks. In this exercise, you'll build a more complex tokenizer for *tweets with hashtags and mentions* using **nltk** and regex. The **nltk.tokenize.TweetTokenizer** class gives you some extra methods and attributes for parsing tweets.\n","*  From **nltk.tokenize**, import **regexp_tokenize** and **TweetTokenizer**\n","\n"],"metadata":{"id":"TSYBjavG91hF"}},{"cell_type":"code","source":["# Import the necessary modules\n","from nltk.tokenize import _\n","from nltk.tokenize import _"],"metadata":{"id":"vkPeYgRD-NVQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tweets = ['This is the #nlp exercise! #python', '#NLP is super fun! <3 #learning', 'Thanks @fitmkmutnb :) #nlp #python']"],"metadata":{"id":"6NF7QXbt-18H"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","*   A regex pattern to define hashtags called pattern1 has been defined for you. Call **regexp_tokenize()** with this hashtag pattern on the first tweet in **tweets** and assign the result to **hashtags**.\n","\n"],"metadata":{"id":"D3wTW4Fi-yxp"}},{"cell_type":"code","source":["# Define a regex pattern to find hashtags: pattern1\n","pattern1 = r\"#\\w+\"\n","\n","# Use the pattern on the first tweet in the tweets list\n","hashtags =\n","print(hashtags)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OMgx-Gzp_PEz","executionInfo":{"status":"ok","timestamp":1721224954330,"user_tz":-420,"elapsed":5,"user":{"displayName":"Supaporn Simcharoen","userId":"08684475074805078308"}},"outputId":"84d31c27-433d-4d03-a266-a94bee47af67"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['#nlp', '#python']\n"]}]},{"cell_type":"markdown","source":["\n","*   Write a new pattern called **pattern2** to match mentions and hashtags.\n","A mention is something like **@fitmkmutnb**.\n","*   Then, call **regexp_tokenize()** with your new hashtag pattern on the last tweet in **tweets** and assign the result to **mentions_hashtags**.\n","\n"],"metadata":{"id":"srNpeoeNAQyh"}},{"cell_type":"code","source":["# Write a pattern that matches both mentions (@) and hashtags\n","pattern2 =\n","# Use the pattern on the last tweet in the tweets list\n","mentions_hashtags =\n","print(mentions_hashtags)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"k53f31czAreZ","executionInfo":{"status":"ok","timestamp":1721225117690,"user_tz":-420,"elapsed":411,"user":{"displayName":"Supaporn Simcharoen","userId":"08684475074805078308"}},"outputId":"824d061a-ba95-4e3d-c536-1cb30e211367"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['@fitmkmutnb', '#nlp', '#python']\n"]}]},{"cell_type":"markdown","source":["\n","*   Create an instance of **TweetTokenizer** called **tknzr** and use it inside a list comprehension to tokenize each tweet into a new list called **all_tokens**.\n","*   To do this, use the **.tokenize()** method of **tknzr**, with t as your iterator variable.\n","\n"],"metadata":{"id":"3RYvGao2Bjtt"}},{"cell_type":"code","source":["# Use the TweetTokenizer to tokenize all tweets into one list\n","tknzr =\n","all_tokens = [___ for t in tweets]\n","print(all_tokens)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jeAa4ffDB1h-","executionInfo":{"status":"ok","timestamp":1721225422755,"user_tz":-420,"elapsed":420,"user":{"displayName":"Supaporn Simcharoen","userId":"08684475074805078308"}},"outputId":"bc163a15-03a5-4a6f-ab11-5e04d4c616a4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[['This', 'is', 'the', '#nlp', 'exercise', '!', '#python'], ['#NLP', 'is', 'super', 'fun', '!', '<3', '#learning'], ['Thanks', '@fitmkmutnb', ':)', '#nlp', '#python']]\n"]}]},{"cell_type":"markdown","source":["**Non-ascii tokenization**\n","\n","*   Practice advanced tokenization by tokenizing some non-ascii based text.\n"],"metadata":{"id":"RgawdgwdEF8p"}},{"cell_type":"code","source":["german_text = \"Wann gehen wir Pizza essen? ðŸ• Und fÃ¤hrst du mit Ãœber? ðŸš•\"\n","print(german_text)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Yo-9OtcAEkUq","executionInfo":{"status":"ok","timestamp":1721227912791,"user_tz":-420,"elapsed":422,"user":{"displayName":"Supaporn Simcharoen","userId":"08684475074805078308"}},"outputId":"b9e5815a-6605-4862-c049-a33392e8fa3c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Wann gehen wir Pizza essen? ðŸ• Und fÃ¤hrst du mit Ãœber? ðŸš•\n"]}]},{"cell_type":"markdown","source":["\n","\n","*  Tokenize all the words in **german_text** using **word_tokenize()**, and print the result.\n","\n","\n","\n"],"metadata":{"id":"96ouCLamE2q5"}},{"cell_type":"code","source":["from nltk.tokenize import word_tokenize\n","import nltk\n","nltk.download('punkt')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6oyyKw4aLSts","executionInfo":{"status":"ok","timestamp":1721227958886,"user_tz":-420,"elapsed":976,"user":{"displayName":"Supaporn Simcharoen","userId":"08684475074805078308"}},"outputId":"5a940c0e-eb78-4a38-e775-c16428d8d75d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":26}]},{"cell_type":"code","source":["# Tokenize and print all words in german_text\n","all_words =\n","print(all_words)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"21yKo0buLIBi","executionInfo":{"status":"ok","timestamp":1721228108853,"user_tz":-420,"elapsed":418,"user":{"displayName":"Supaporn Simcharoen","userId":"08684475074805078308"}},"outputId":"dcaed30e-ba36-4dc9-d978-b2b7b8bcc785"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['Wann', 'gehen', 'wir', 'Pizza', 'essen', '?', 'ðŸ•', 'Und', 'fÃ¤hrst', 'du', 'mit', 'Ãœber', '?', 'ðŸš•']\n"]}]},{"cell_type":"markdown","source":["\n","\n","*  Tokenize only the capital words\n","\n"],"metadata":{"id":"LGiM6KyHLwvo"}},{"cell_type":"code","source":["# Tokenize and print only capital words\n","capital_words =\n","___(german_text, capital_words)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"smGG6rAnMAPq","executionInfo":{"status":"ok","timestamp":1721228117003,"user_tz":-420,"elapsed":428,"user":{"displayName":"Supaporn Simcharoen","userId":"08684475074805078308"}},"outputId":"0e91162d-2eaa-4b52-8e05-7f658d10865f"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['Wann', 'Pizza', 'Und', 'Ãœber']"]},"metadata":{},"execution_count":29}]},{"cell_type":"markdown","source":["\n","\n","*   Tokenize only the emoji in **german_text**. The pattern using the unicode ranges for emoji given in the assignment text has been written for you. Your job is to use **regexp_tokenize()** to tokenize the emoji.\n","\n"],"metadata":{"id":"AX9z9iwRNP06"}},{"cell_type":"code","source":["# Tokenize and print only emoji\n","emoji = \"['\\U0001F300-\\U0001F5FF'|'\\U0001F600-\\U0001F64F'|'\\U0001F680-\\U0001F6FF'|'\\u2600-\\u26FF\\u2700-\\u27BF']\"\n","___(german_text, emoji)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hq6XY1MWTD4E","executionInfo":{"status":"ok","timestamp":1721229943380,"user_tz":-420,"elapsed":460,"user":{"displayName":"Supaporn Simcharoen","userId":"08684475074805078308"}},"outputId":"2303b073-4d17-430e-f0b8-1276bb73ac0a"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['ðŸ•', 'ðŸš•']"]},"metadata":{},"execution_count":31}]}]}